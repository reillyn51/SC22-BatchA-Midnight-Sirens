{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "LGtV5-Jc6zJz"
   },
   "source": [
    "# Classifying handwritten digits (0-9) with neural networks \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "KU8e-P7CDkvN"
   },
   "source": [
    "<h1>Getting Set Up</h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "kWiyySymuN07"
   },
   "source": [
    "Random is the default python library for generating random numbers.\n",
    "\n",
    "PyTorch, or torch, is the python deep learning library we use for our neural networks. \n",
    "\n",
    "Torchvision is for computer vision specific functions such as transforming images and image datasets. \n",
    "\n",
    "Matplotlib is used for graphing figures with data, whether it be scatterplots, heatmaps, lineplots, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "id": "LtoR_ZZwuDQj"
   },
   "outputs": [
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "m2tj_3PsuRkh"
   },
   "source": [
    "<h1>Preprocessing our Images</h1>\n",
    "\n",
    "The transform that we will perform on our dataset is first converting all images to tensors. Tensors are the built in array datatype in pytorch, like numpy arrays. If interested, learn about why they are useful in keeping track of gradients here:\n",
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html .\n",
    "\n",
    "Converting to a tensor also converts an image with pixel values from 0 to 255 to a matrix with numbers from 0 to 1. \n",
    "\n",
    "In addition, we are normalizing the data to a range between -1 and 1. If the range before is [0,1], subtracting 0.5 will give us [-0.5,0.5] and dividing by 0.5 will make the range wider to [-1,1]. torch.Normalize subtracts the first parameter from all the values in the image and divides by the second parameter. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "id": "LSyJL6QIuJM4"
   },
   "outputs": [
   ],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "  torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "MMEm9idPuYWq"
   },
   "source": [
    "# Applying the transforms\n",
    "\n",
    "We are using the datasets.MNIST function from torchvision to import the dataset. \n",
    "\n",
    "Parameters to the function:\n",
    "\n",
    "The root parameter sets the directory that we import the data to (and create it if it doesn't exist.)\n",
    "\n",
    "The train parameter determines if we are importing training or testing fashion MNIST dataset. \n",
    "\n",
    "The transform parameter determines the transforms we apply during preprocessing, which were defined above.\n",
    "\n",
    "download=True gives the function permission to download the data into the directory if it doesn't exist there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "id": "Q_fPpFanuNAa",
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "trainset = torchvision.datasets.MNIST('train_set', download=True, train=True, transform=transform) # downloads to train_set\n",
    "valset = torchvision.datasets.MNIST('test_set', download=True, train=False, transform=transform) # downloads to test_set\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # batch is the number of images to consider at a time\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "0T4Y-JVy4WUy"
   },
   "source": [
    "## Visualization !!!\n",
    "\n",
    "For 10 iterations, select a random index from zero to the length of the training dataset. Display each image in a 2 by 5 subplot as the 1st, 2nd, 3rd etc... image in the plot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "collapsed": false,
    "id": "AwWg6nMm4gdg",
    "outputId": "bfd04d34-6ca8-43bc-8830-e3978ec1316e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    rand_idx = random.randint(0, len(trainset)-1) # Pick an index for a random image\n",
    "    plt.subplot(2, 5, i+1) # 2 * 5 images in the grid, display 10 total\n",
    "    plt.axis('off')\n",
    "    # [1, 28, 28] -> [28, 28]\n",
    "    plt.imshow(trainset[rand_idx][0].squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "pc8-Fbf4Dkvb"
   },
   "source": [
    "<h1>Creating the Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_xDFVY4muq_1"
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "datasets docs - https://pytorch.org/vision/stable/datasets.html\n",
    "\n",
    "The input size is the pixel size of the images, each of which is 28 by 28.\n",
    "\n",
    "The number of classifcation (num_classes) is 10 because there are 10 possible classifications the model can make, such as 0, 1, 2 ... 9. \n",
    "\n",
    "The hidden_sizes is the number of neurons in the hidden layer of the neural network. The input size and output sizes are always fixed (input size is related to number of pixels and output size is the number of classes) but the optimal hidden layer sizes can be determined only through experimentation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "id": "Bg20RWXHuW2W"
   },
   "outputs": [
   ],
   "source": [
    "input_size = 28 * 28 # width times height of the image (number of pixels)\n",
    "hidden_sizes = [128, 32] # this is the sizes of the hidden layers. The sizes are relativly arbitrary\n",
    "num_class = 10 # one label for each digit (0-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "E9WnnZwZxGWq"
   },
   "source": [
    "GPU boost training time. Why? Because it lets us do many operations at the same time in a parallelized sort of way. \n",
    "\n",
    "CUDA is the API that we will use for GPU training. If CUDA is available we want to use it, and otherwise use the CPU. Google colab comes with a built in GPU for use so make sure to activate it by going to Runtime->Change runtime type->GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "id": "ATekqXGWxKBs"
   },
   "outputs": [
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "hRpV2GTwvBYZ"
   },
   "source": [
    "Build a neural network in pytorch with two hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "QtFMastrDkvf"
   },
   "source": [
    "So torch.nn.Sequential pretty much compiles a group of layers into one network, and runs them sequentially for predictions. All of the layers that we are using for this are fully connected layers. The input size is 28 by 28 because we are flattening out the 28 by 28 image into 28*28=784 numbers. We are adding a activation function ReLU after that. ReLU(x) = max(x,0) so relu turns negative values to zero and positive values stay the same. \n",
    "\n",
    "After the first layer, we take the number of outputs of the first layer as the number of inputs into the second layer. And an arbitrary number of outputs for the second layer that we decide. \n",
    "\n",
    "Notice how we have an activation function after each layer. \n",
    "\n",
    "Then the third layer you code yourself, view the instructions below. \n",
    "\n",
    "Finally, we have an output activation function. The 10 numbers that are outputted from our previous layer can be any numbers from negative infinity to infinity. We want every class output to be from zero to one, like a probability. The softmax activation function turns the output into a probability for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "id": "iWS_Hzt-uhNP"
   },
   "outputs": [
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    # [(64) batch_size, (768) width x height] -> [(64) batch_size, (128) hidden_size #1]\n",
    "    torch.nn.Linear(input_size, hidden_sizes[0]),\n",
    "    torch.nn.ReLU(), # activation function\n",
    "    # [(64) batch_size, (128) hidden_size #1] -> [(64) batch_size, (32) hidden_size #2]\n",
    "    torch.nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    torch.nn.ReLU(), # activation function\n",
    "    # [(64) batch_size, (32) hidden_size #2] -> [(64) batch_size, (10) num_class]\n",
    "\n",
    "    #EXERCISE: Define the third layer as taking in the output size of the second layer and outputting the number of classes. \n",
    "\n",
    "    # output activation function - the hidden layer functions don't work for optimization\n",
    "    # LogSoftmax because it is better at gradient optimization\n",
    "    torch.nn.LogSoftmax(dim=-1) # apply LogSoftmax to the last layer (num_class)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "wqegJQ24Dkvg"
   },
   "source": [
    "We have to transfer the model to the GPU device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "zqtJo49FxLH-",
    "outputId": "3b8bc56d-ab7f-4a57-da50-4eb6d2f4651e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "N1UvEyWzDkvi"
   },
   "source": [
    "<h1>Training the model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1HqqlulNwyqb"
   },
   "source": [
    "## Training our Model\n",
    "\n",
    "The training process goes somewhat like this\n",
    "go through the dataset [epoch] times<br>\n",
    "&ensp;  go through each image in the dataset<br>\n",
    "&ensp;&ensp; transfer inputs and labels to GPU<br>\n",
    "&ensp;&ensp; get prediction for input<br>\n",
    "&ensp;&ensp; check if prediction matches label, get loss<br>\n",
    "&ensp;&ensp; see which direction you have to change the weights<br>\n",
    "&ensp;&ensp; actually change weights using optimizer and learning rate\n",
    "&ensp;&ensp; Set the directions back to zero (optim.zero_grad())<br>\n",
    "&ensp;&ensp; add loss to total loss until reset<br>\n",
    "&ensp;&ensp; after some iterations, print out loss and reset\n",
    "\n",
    "Few more things to note. NLLLoss is the default loss function for softmax (probabilities that are far away from the true probabilities are penalized). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "wpYVqK89Dkvj"
   },
   "source": [
    "NLLLoss docs - https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html?highlight=nllloss#torch.nn.NLLLoss\n",
    "Optim docs - https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "s-B0laEkwlst",
    "outputId": "b6c2fd03-4761-42fe-ad5f-7087096b3a88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.6979258381989973\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      6\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m#EXERCISE: transfer images and labels to GPU. \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# Formats the image to be a usable 1d array.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m#   [(64) batch size, 1, 28, 28] -> [(64) batch size, 768]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mview(images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# Resets the optimizer for each training step\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_env/lib/python3.8/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_env/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_env/lib/python3.8/site-packages/torchvision/transforms/transforms.py:270\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_env/lib/python3.8/site-packages/torchvision/transforms/functional.py:355\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    352\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    354\u001b[0m dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 355\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(std, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (std \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model training for 8 times\n",
    "criterion = torch.nn.NLLLoss() # Loss object to find back propagation\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9) # Optimizer\n",
    "epochs = 8\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        #EXERCISE: transfer images and labels to GPU. \n",
    "\n",
    "        # Formats the image to be a usable 1d array.\n",
    "        #   [(64) batch size, 1, 28, 28] -> [(64) batch size, 768]\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Resets the optimizer for each training step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Finds error then runs back propagation\n",
    "        output = model(images)\n",
    "        #EXERCISE: calculate the loss by passing the outputs and the labels into the lsos function. \n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updates model weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Keeps track of error to allow visualization of progress\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "S_58fHPtDkvk"
   },
   "source": [
    "## Evaluation Loop\n",
    "&ensp; Iterate through every batch in the dataset<br>\n",
    "&ensp;&ensp; Get the prediction of every image in the batch<br>\n",
    "&ensp;&ensp; Add the number of images to the total<br>\n",
    "&ensp;&ensp; Add the number of correctly classified images to a counter<br>\n",
    "&ensp;&ensp; Get the accuracy through correct/total.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "Ppm0FK4xyJQR",
    "outputId": "687c617f-975b-4636-f2e6-f3f321837f1b"
   },
   "outputs": [
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for images, labels in valloader:\n",
    "        #EXERCISE: Transfer the inputs and labels to the GPU\n",
    "        #EXERCISE: calculate outputs by running images through the network\n",
    "        \n",
    "        images = images.view(images.shape[0], -1)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FU9DtjN9Dkvm"
   },
   "source": [
    "<h1>Using the Model!</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ffF-vY365-Dn"
   },
   "source": [
    "Got your own image to run prediction on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "collapsed": false,
    "id": "9CrgNFgI3xYt",
    "outputId": "3b10d270-b394-4a36-f944-2b57006315cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe4799194f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get and show a sample image\n",
    "image = valset[0][0] # shape: [(1) batch_size, 28, 28]\n",
    "plt.subplot()\n",
    "plt.axis('off')\n",
    "# [1, 28, 28] -> [28, 28] - makes the image readable\n",
    "plt.imshow(image.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "q2ZVmfom6XsO"
   },
   "source": [
    "Run Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "2hxLIQGI6HHa",
    "outputId": "53825ba4-27c7-424b-d6c9-63a7eccbdcf2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# cast input to device\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m     log_preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(image) \u001b[38;5;66;03m# runs the model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# post processes the image into probabilities of it being each digit\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#   Math: probabilites were natural logged, so torch.exp() performs e^(log_preds)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(log_preds)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# [(1) batch_size, 28, 28]-> [1(batch size), 784]\n",
    "image = image.view(1, 784)\n",
    "with torch.no_grad():\n",
    "    # cast input to device\n",
    "    image = image.to(device)\n",
    "\n",
    "    log_preds = model(image) # runs the model\n",
    "\n",
    "# post processes the image into probabilities of it being each digit\n",
    "#   Math: probabilites were natural logged, so torch.exp() performs e^(log_preds)\n",
    "preds = torch.exp(log_preds)\n",
    "probab = list(preds.cpu().numpy()[0])\n",
    "\n",
    "# the index, this time, is the same as the output, so we can just grab and print it\n",
    "pred_label = probab.index(max(probab)) # get index of highest num (highest probability)\n",
    "print(f\"Prediction: {pred_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
   ],
   "name": "Digits_Classifier.ipynb",
   "provenance": [
   ]
  },
  "kernelspec": {
   "display_name": "Python (cv_env)",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "cv_env",
   "resource_dir": "/projects/9f552ddb-0255-4438-a179-8f8c475cfce3/.local/share/jupyter/kernels/cv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}