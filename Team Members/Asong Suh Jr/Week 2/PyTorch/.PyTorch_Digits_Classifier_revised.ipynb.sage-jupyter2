{"backend_state":"running","connection_file":"/projects/9f552ddb-0255-4438-a179-8f8c475cfce3/.local/share/jupyter/runtime/kernel-483c0558-b234-475a-8907-1f5bdb50ad3e.json","kernel":"cv_env","kernel_error":"","kernel_state":"busy","kernel_usage":{"cpu":0,"memory":0},"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Digits_Classifier.ipynb","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1655145331341,"exec_count":10,"id":"b9ffa7","input":"correct = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for images, labels in valloader:\n        #EXERCISE: Transfer the inputs and labels to the GPU\n        #EXERCISE: calculate outputs by running images through the network\n        \n        images = images.view(images.shape[0], -1)\n        # calculate outputs by running images through the network\n        outputs = model(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')","kernel":"cv_env","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ppm0FK4xyJQR","outputId":"687c617f-975b-4636-f2e6-f3f321837f1b"},"no_halt":true,"output":{"0":{"ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mview(images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# calculate outputs by running images through the network\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(images)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# the class with the highest energy is what we choose as prediction\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}},"pos":25,"start":1655145331298,"state":"done","type":"cell"}
{"cell_type":"code","end":1655145331402,"exec_count":11,"id":"3d9699","input":"# get and show a sample image\nimage = valset[0][0] # shape: [(1) batch_size, 28, 28]\nplt.subplot()\nplt.axis('off')\n# [1, 28, 28] -> [28, 28] - makes the image readable\nplt.imshow(image.squeeze(0))","kernel":"cv_env","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"9CrgNFgI3xYt","outputId":"3b10d270-b394-4a36-f944-2b57006315cb"},"no_halt":true,"output":{"0":{"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f4d72f15a30>"},"exec_count":11},"1":{"data":{"image/png":"0f7e39a34029b0ef035d89b6944b3de6d78ad3c6","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"}}},"pos":28,"start":1655145331347,"state":"done","type":"cell"}
{"cell_type":"code","end":1655145331443,"exec_count":12,"id":"7387e0","input":"# [(1) batch_size, 28, 28]-> [1(batch size), 784]\nimage = image.view(1, 784)\nwith torch.no_grad():\n    # cast input to device\n    image = image.to(device)\n\n    log_preds = model(image) # runs the model\n\n# post processes the image into probabilities of it being each digit\n#   Math: probabilites were natural logged, so torch.exp() performs e^(log_preds)\npreds = torch.exp(log_preds)\nprobab = list(preds.cpu().numpy()[0])\n\n# the index, this time, is the same as the output, so we can just grab and print it\npred_label = probab.index(max(probab)) # get index of highest num (highest probability)\nprint(f\"Prediction: {pred_label}\")","kernel":"cv_env","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2hxLIQGI6HHa","outputId":"53825ba4-27c7-424b-d6c9-63a7eccbdcf2"},"no_halt":true,"output":{"0":{"ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# cast input to device\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m     log_preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(image) \u001b[38;5;66;03m# runs the model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# post processes the image into probabilities of it being each digit\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#   Math: probabilites were natural logged, so torch.exp() performs e^(log_preds)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(log_preds)\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}},"pos":30,"start":1655145331415,"state":"done","type":"cell"}
{"cell_type":"code","end":1655145586366,"exec_count":3,"id":"1b4ea7","input":"import random\nimport torch\nimport torchvision\nimport matplotlib.pyplot as plt","kernel":"cv_env","metadata":{"id":"LtoR_ZZwuDQj"},"pos":3,"start":1655145586355,"state":"done","type":"cell"}
{"cell_type":"code","end":1655145589093,"exec_count":4,"id":"112805","input":"transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n  torchvision.transforms.Normalize((0.5,), (0.5,)),\n])","kernel":"cv_env","metadata":{"id":"LSyJL6QIuJM4"},"pos":5,"start":1655145589089,"state":"done","type":"cell"}
{"cell_type":"code","end":1655145592066,"exec_count":5,"id":"c770d8","input":"trainset = torchvision.datasets.MNIST('train_set', download=True, train=True, transform=transform) # downloads to train_set\nvalset = torchvision.datasets.MNIST('test_set', download=True, train=False, transform=transform) # downloads to test_set\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # batch is the number of images to consider at a time\nvalloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)","kernel":"cv_env","metadata":{"id":"Q_fPpFanuNAa"},"pos":7,"scrolled":true,"start":1655145591843,"state":"done","type":"cell"}
{"cell_type":"code","end":1655145595120,"exec_count":6,"id":"fde9da","input":"for i in range(10):\n    rand_idx = random.randint(0, len(trainset)-1) # Pick an index for a random image\n    plt.subplot(2, 5, i+1) # 2 * 5 images in the grid, display 10 total\n    plt.axis('off')\n    # [1, 28, 28] -> [28, 28]\n    plt.imshow(trainset[rand_idx][0].squeeze(0))","kernel":"cv_env","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":207},"id":"AwWg6nMm4gdg","outputId":"bfd04d34-6ca8-43bc-8830-e3978ec1316e"},"output":{"0":{"data":{"image/png":"3827ace82e9f53662b0dfca8fb3d066043a99c77","text/plain":"<Figure size 432x288 with 10 Axes>"},"metadata":{"needs_background":"light"}}},"pos":9,"scrolled":true,"start":1655145594698,"state":"done","type":"cell"}
{"cell_type":"code","end":1655145601990,"exec_count":8,"id":"e55997","input":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","kernel":"cv_env","metadata":{"id":"ATekqXGWxKBs"},"pos":14,"start":1655145601972,"state":"done","type":"cell"}
{"cell_type":"code","end":1655145817222,"exec_count":16,"id":"9da9a4","input":"input_size = 28 * 28 # width times height of the image (number of pixels)\nhidden_sizes = [128, 32, 10] # this is the sizes of the hidden layers. The sizes are relatively arbitrary\nnum_class = 10 # one label for each digit (0-9)","kernel":"cv_env","metadata":{"id":"Bg20RWXHuW2W"},"pos":12,"start":1655145817211,"state":"done","type":"cell"}
{"cell_type":"code","end":1655146008346,"exec_count":23,"id":"31d21f","input":"model = torch.nn.Sequential(\n    # [(64) batch_size, (768) width x height] -> [(64) batch_size, (128) hidden_size #1]\n    torch.nn.Linear(input_size, hidden_sizes[0]),\n    torch.nn.ReLU(), # activation function\n    # [(64) batch_size, (128) hidden_size #1] -> [(64) batch_size, (32) hidden_size #2]\n    torch.nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n    torch.nn.ReLU(), # activation function\n    # [(64) batch_size, (32) hidden_size #2] -> [(64) batch_size, (10) num_class]\n\n    #EXERCISE: Define the third layer as taking in the output size of the second layer and outputting the number of classes. \n    torch.nn.Linear(hidden_sizes[1],hidden_sizes[2]),\n    torch.nn.ReLU(),# output activation function - the hidden layer functions don't work for optimization\n    # LogSoftmax because it is better at gradient optimization\n    torch.nn.LogSoftmax(dim=-1) # apply LogSoftmax to the last layer (num_class)\n)","kernel":"cv_env","metadata":{"id":"iWS_Hzt-uhNP"},"output":{"0":{"ename":"TypeError","evalue":"tuple is not a Module subclass","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# [(64) batch_size, (768) width x height] -> [(64) batch_size, (128) hidden_size #1]\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReLU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# activation function\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# [(64) batch_size, (128) hidden_size #1] -> [(64) batch_size, (32) hidden_size #2]\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReLU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# activation function\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# [(64) batch_size, (32) hidden_size #2] -> [(64) batch_size, (10) num_class]\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#EXERCISE: Define the third layer as taking in the output size of the second layer and outputting the number of classes. \u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhidden_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# output activation function - the hidden layer functions don't work for optimization\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# LogSoftmax because it is better at gradient optimization\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLogSoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# apply LogSoftmax to the last layer (num_class)\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/cv_env/lib/python3.8/site-packages/torch/nn/modules/container.py:91\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(args):\n\u001b[0;32m---> 91\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/cv_env/lib/python3.8/site-packages/torch/nn/modules/module.py:381\u001b[0m, in \u001b[0;36mModule.add_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Adds a child module to the current module.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03mThe module can be accessed as an attribute using the given name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m    module (Module): child module to be added to the module.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, Module) \u001b[38;5;129;01mand\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not a Module subclass\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    382\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtypename(module)))\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, torch\u001b[38;5;241m.\u001b[39m_six\u001b[38;5;241m.\u001b[39mstring_classes):\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule name should be a string. Got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    385\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtypename(name)))\n","\u001b[0;31mTypeError\u001b[0m: tuple is not a Module subclass"]}},"pos":17,"start":1655146008337,"state":"done","type":"cell"}
{"cell_type":"code","end":1655146013881,"exec_count":24,"id":"963191","input":"model.to(device)","kernel":"cv_env","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zqtJo49FxLH-","outputId":"3b8bc56d-ab7f-4a57-da50-4eb6d2f4651e"},"no_halt":true,"output":{"0":{"data":{"text/plain":"Sequential(\n  (0): Linear(in_features=784, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=32, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=32, out_features=10, bias=True)\n  (5): ReLU()\n  (6): LogSoftmax(dim=-1)\n)"},"exec_count":24}},"pos":19,"start":1655146013875,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":37,"id":"c158fc","input":"# Model training for 8 times\ncriterion = torch.nn.NLLLoss() # Loss object to find back propagation\noptimizer = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9) # Optimizer\nepochs = 8\nfor e in range(epochs):\n    running_loss = 0\n    for images, labels in trainloader:\n        #EXERCISE: transfer images and labels to GPU. \n\n        # Formats the image to be a usable 1d array.\n        #   [(64) batch size, 1, 28, 28] -> [(64) batch size, 768]\n        images = images.view(images.shape[0], -1)\n    \n        # Resets the optimizer for each training step\n        optimizer.zero_grad()\n        \n        # Finds error then runs back propagation\n        output = model(images)\n        #EXERCISE: calculate the loss by passing the outputs and the labels into the lsos function. \n        loss = criterion(output, labels)\n        loss.backward()\n        \n        # Updates model weights\n        optimizer.step()\n        \n        # Keeps track of error to allow visualization of progress\n        running_loss += loss.item()\n    else:\n        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))","kernel":"cv_env","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-B0laEkwlst","outputId":"b6c2fd03-4761-42fe-ad5f-7087096b3a88"},"output":{"0":{"name":"stdout","text":"Epoch 0 - Training loss: 0.9742576981118238\n"},"1":{"name":"stdout","text":"Epoch 1 - Training loss: 0.35417287567181627\n"},"2":{"name":"stdout","text":"Epoch 2 - Training loss: 0.22867737123881704\n"},"3":{"name":"stdout","text":"Epoch 3 - Training loss: 0.1858579717925998\n"},"4":{"name":"stdout","text":"Epoch 4 - Training loss: 0.15559280729258873\n"}},"pos":23,"start":1655146859192,"state":"busy","type":"cell"}
{"cell_type":"markdown","id":"0923be","input":"# Classifying handwritten digits (0-9) with neural networks \n\n","metadata":{"id":"LGtV5-Jc6zJz"},"pos":0,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"0ea412","input":"## Training our Model\n\nThe training process goes somewhat like this\ngo through the dataset [epoch] times<br>\n&ensp;  go through each image in the dataset<br>\n&ensp;&ensp; transfer inputs and labels to GPU<br>\n&ensp;&ensp; get prediction for input<br>\n&ensp;&ensp; check if prediction matches label, get loss<br>\n&ensp;&ensp; see which direction you have to change the weights<br>\n&ensp;&ensp; actually change weights using optimizer and learning rate\n&ensp;&ensp; Set the directions back to zero (optim.zero_grad())<br>\n&ensp;&ensp; add loss to total loss until reset<br>\n&ensp;&ensp; after some iterations, print out loss and reset\n\nFew more things to note. NLLLoss is the default loss function for softmax (probabilities that are far away from the true probabilities are penalized). \n\n","metadata":{"id":"1HqqlulNwyqb"},"pos":21,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"13b24a","input":"# Applying the transforms\n\nWe are using the datasets.MNIST function from torchvision to import the dataset. \n\nParameters to the function:\n\nThe root parameter sets the directory that we import the data to (and create it if it doesn't exist.)\n\nThe train parameter determines if we are importing training or testing fashion MNIST dataset. \n\nThe transform parameter determines the transforms we apply during preprocessing, which were defined above.\n\ndownload=True gives the function permission to download the data into the directory if it doesn't exist there. \n\n","metadata":{"id":"MMEm9idPuYWq"},"pos":6,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"16564e","input":"Got your own image to run prediction on?","metadata":{"id":"ffF-vY365-Dn"},"pos":27,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"1c679f","input":"## Visualization !!!\n\nFor 10 iterations, select a random index from zero to the length of the training dataset. Display each image in a 2 by 5 subplot as the 1st, 2nd, 3rd etc... image in the plot.\n\n","metadata":{"id":"0T4Y-JVy4WUy"},"pos":8,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"51960b","input":"GPU boost training time. Why? Because it lets us do many operations at the same time in a parallelized sort of way. \n\nCUDA is the API that we will use for GPU training. If CUDA is available we want to use it, and otherwise use the CPU. Google colab comes with a built in GPU for use so make sure to activate it by going to Runtime->Change runtime type->GPU","metadata":{"id":"E9WnnZwZxGWq"},"pos":13,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"57a92a","input":"We have to transfer the model to the GPU device. \n\n","metadata":{"id":"wqegJQ24Dkvg"},"pos":18,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"5d828d","input":"NLLLoss docs - https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html?highlight=nllloss#torch.nn.NLLLoss\nOptim docs - https://pytorch.org/docs/stable/optim.html\n\n","metadata":{"id":"wpYVqK89Dkvj"},"pos":22,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"6378d3","input":"<h1>Getting Set Up</h1>\n\n","metadata":{"id":"KU8e-P7CDkvN"},"pos":1,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"723fcc","input":"Build a neural network in pytorch with two hidden layers","metadata":{"id":"hRpV2GTwvBYZ"},"pos":15,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"74bce2","input":"So torch.nn.Sequential pretty much compiles a group of layers into one network, and runs them sequentially for predictions. All of the layers that we are using for this are fully connected layers. The input size is 28 by 28 because we are flattening out the 28 by 28 image into 28*28=784 numbers. We are adding a activation function ReLU after that. ReLU(x) = max(x,0) so relu turns negative values to zero and positive values stay the same. \n\nAfter the first layer, we take the number of outputs of the first layer as the number of inputs into the second layer. And an arbitrary number of outputs for the second layer that we decide. \n\nNotice how we have an activation function after each layer. \n\nThen the third layer you code yourself, view the instructions below. \n\nFinally, we have an output activation function. The 10 numbers that are outputted from our previous layer can be any numbers from negative infinity to infinity. We want every class output to be from zero to one, like a probability. The softmax activation function turns the output into a probability for each class. \n\n","metadata":{"id":"QtFMastrDkvf"},"pos":16,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"852863","input":"<h1>Using the Model!</h1>","metadata":{"id":"FU9DtjN9Dkvm"},"pos":26,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"95dc56","input":"Random is the default python library for generating random numbers.\n\nPyTorch, or torch, is the python deep learning library we use for our neural networks. \n\nTorchvision is for computer vision specific functions such as transforming images and image datasets. \n\nMatplotlib is used for graphing figures with data, whether it be scatterplots, heatmaps, lineplots, etc...\n\n","metadata":{"id":"kWiyySymuN07"},"pos":2,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"a72c6c","input":"<h1>Training the model</h1>\n\n","metadata":{"id":"N1UvEyWzDkvi"},"pos":20,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"c083d3","input":"## Evaluation Loop\n&ensp; Iterate through every batch in the dataset<br>\n&ensp;&ensp; Get the prediction of every image in the batch<br>\n&ensp;&ensp; Add the number of images to the total<br>\n&ensp;&ensp; Add the number of correctly classified images to a counter<br>\n&ensp;&ensp; Get the accuracy through correct/total.<br>\n","metadata":{"id":"S_58fHPtDkvk"},"pos":24,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"cbbdc1","input":"## Hyperparameters\n\ndatasets docs - https://pytorch.org/vision/stable/datasets.html\n\nThe input size is the pixel size of the images, each of which is 28 by 28.\n\nThe number of classifcation (num_classes) is 10 because there are 10 possible classifications the model can make, such as 0, 1, 2 ... 9. \n\nThe hidden_sizes is the number of neurons in the hidden layer of the neural network. The input size and output sizes are always fixed (input size is related to number of pixels and output size is the number of classes) but the optimal hidden layer sizes can be determined only through experimentation. \n\n","metadata":{"id":"_xDFVY4muq_1"},"pos":11,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"ee1a7d","input":"Run Prediction","metadata":{"id":"q2ZVmfom6XsO"},"pos":29,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"f61a09","input":"<h1>Creating the Model</h1>\n\n","metadata":{"id":"pc8-Fbf4Dkvb"},"pos":10,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"f62896","input":"<h1>Preprocessing our Images</h1>\n\nThe transform that we will perform on our dataset is first converting all images to tensors. Tensors are the built in array datatype in pytorch, like numpy arrays. If interested, learn about why they are useful in keeping track of gradients here:\nhttps://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html .\n\nConverting to a tensor also converts an image with pixel values from 0 to 255 to a matrix with numbers from 0 to 1. \n\nIn addition, we are normalizing the data to a range between -1 and 1. If the range before is [0,1], subtracting 0.5 will give us [-0.5,0.5] and dividing by 0.5 will make the range wider to [-1,1]. torch.Normalize subtracts the first parameter from all the values in the image and divides by the second parameter. \n\n","metadata":{"id":"m2tj_3PsuRkh"},"pos":4,"state":"done","type":"cell"}
{"id":0,"time":1655145568833,"type":"user"}
{"last_load":1655145112832,"type":"file"}